Week_1
- Как мы представляем параметры НС (X,W,Y
- Стоит ли строить НС с использованием циклов?
- Что такое Ground Truth?
- Почему мы используем LogLoss вместо MAE, MSE для задачи классификации?
- Различия между Loss и Cost Functions?
- Представь логистическую регрессию как НС. Как осуществить прямое и обратное распространение в такой сети?
- Чему равны следующие матрицы: dZ, dW, dB
- Что такое BroadCasting in NumPy?
- Связь LogLoss и Метода Максимального Правдоподобия (Maximum Likelihood Estimation/Method)
- Что вычисляет нейрон?
Week_2-Week_3
- Как определить число слоев в НС? Считается ли Input Layer?
- Какую размерность всегда будут иметь параметры (матрицы) W, B в многослойной НС?
- Так ли хороша ФА сигмоида в скрытых слоях?
- К чему приведёт использование только линейных ФА?
- Алгоритм градиентного спуска
- Что будет если инициализировать веса и баесы нулями?
- Почему лучше инициализировать веса НС малыми значениями?
Week_4
- При программировании НС стоит ли сохранять значения Z, A, B?
- Что можно отнести к гиперпараметрам НС?
- Что обычно вычисляют глубокие слои НС?
